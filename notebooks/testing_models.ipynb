{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b4d61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaurav/Documents/projects/OpenDeepResearch/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from typing import Literal\n",
    "\n",
    "from src.config import MODEL_CONFIG\n",
    "\n",
    "\n",
    "def get_llm(\n",
    "    model_name: str,\n",
    "    temperature: float = 0,\n",
    "    reasoning: bool | Literal[\"low\", \"medium\", \"high\"] | None = None,\n",
    "    **kwargs,\n",
    ") -> ChatOllama:\n",
    "    \"\"\"\n",
    "    Returns a ChatOllama instance with the specified model name and parameters\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): The name of the Ollama model to use.\n",
    "    - temperature (float): The temperature setting for the model.\n",
    "    - reasoning (bool|str): Whether to enable reasoning mode. Set it to a string for,\n",
    "       'gpt-oss' models and valid values are 'low', 'medium', 'high'.\n",
    "    - **kwargs: Additional keyword arguments to pass to the ChatOllama constructor.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        model_name.startswith(\"gemma3\")\n",
    "        or model_name.startswith(\"llama3.1\")\n",
    "        or model_name.startswith(\"granite4\")\n",
    "        or model_name.startswith(\"mistral\")\n",
    "    ):\n",
    "        reasoning = None  # Gemma3 models do not support reasoning mode\n",
    "\n",
    "    return ChatOllama(\n",
    "        model=model_name,\n",
    "        validate_model_on_init=True,\n",
    "        temperature=temperature,\n",
    "        reasoning=reasoning,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    temperature: float = MODEL_CONFIG[\"temperature\"],\n",
    "    reasoning: bool | Literal[\"low\", \"medium\", \"high\"] | None = MODEL_CONFIG[\n",
    "        \"reasoning\"\n",
    "    ],\n",
    "    **kwargs,\n",
    ") -> ChatOllama:\n",
    "    \"\"\"\n",
    "    Returns a default ChatOllama instance with predefined model configuration.\n",
    "    Parameters:\n",
    "    - temperature (float): The temperature setting for the model.\n",
    "    - reasoning (bool|str): Whether to enable reasoning mode. Set it to a string for\n",
    "       'gpt-oss' models and valid values are 'low', 'medium', 'high'.\n",
    "    Returns:\n",
    "    - ChatOllama: Configured ChatOllama instance.\n",
    "    \"\"\"\n",
    "    return get_llm(\n",
    "        model_name=MODEL_CONFIG[\"model_name\"],\n",
    "        temperature=temperature,\n",
    "        reasoning=reasoning,\n",
    "        **kwargs,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74dd8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'nemotron-3-nano:30b'\n",
    "reasoning = None\n",
    "model = ChatOllama(\n",
    "        model=model_name,\n",
    "        validate_model_on_init=True,\n",
    "        temperature=0,\n",
    "        reasoning=reasoning,\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e17a674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea9f138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! Here's one for you:\\n\\n**Why don't scientists trust atoms anymore?**\\n\\nBecause they *make up* everything! ðŸ˜„\\n\\nHope that gave you a chuckle! If you want another, just let me know.\", additional_kwargs={}, response_metadata={'model': 'nemotron-3-nano:30b', 'created_at': '2025-12-27T23:34:09.335725Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1531813250, 'load_duration': 103667000, 'prompt_eval_count': 20, 'prompt_eval_duration': 315941458, 'eval_count': 64, 'eval_duration': 790466000, 'logprobs': None, 'model_name': 'nemotron-3-nano:30b', 'model_provider': 'ollama'}, id='lc_run--019b6229-88b9-7a82-80f4-a88be4005446-0', usage_metadata={'input_tokens': 20, 'output_tokens': 64, 'total_tokens': 84})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f1ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opendeepresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
